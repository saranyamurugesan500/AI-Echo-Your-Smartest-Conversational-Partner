# sentiment_dashboard_rewrite.py
"""
Rewritten Sentiment Dashboard
- Keeps original features (wordcloud, LDA, heatmaps, etc.)
- Fixes selection/caching/rendering bugs
- Shows full question text in selectbox
- Uses a handler mapping for all 10 questions
- Ensures cached functions depend on the selected question (no stale output)
"""

import os
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import nltk
import re

# NLTK stopwords
nltk.download("stopwords", quiet=True)
from nltk.corpus import stopwords
EN_STOPWORDS = set(stopwords.words("english"))

# Streamlit config
st.set_page_config(page_title="Sentiment Dashboard", layout="wide")
st.title("ðŸ“Š Sentiment Analysis Dashboard")
st.markdown("Interactive dashboard answering the 10 key sentiment questions.")

# -------------------------
# Sidebar: Dataset & Options
# -------------------------
st.sidebar.header("Dataset & Options")
data_path = st.sidebar.text_input("Path to CSV dataset", "chatgpt_style_reviews.csv")
load_button = st.sidebar.button("Load dataset")

@st.cache_data
def load_df(path):
    if not os.path.exists(path):
        return pd.DataFrame()
    df = pd.read_csv(path)
    return df

# load (pressing load is optional â€“ both branches call load_df)
if load_button:
    df = load_df(data_path)
else:
    df = load_df(data_path)

if df.empty:
    st.warning(f"Dataset not found at: {data_path}. Upload your CSV or provide the correct path.")
    st.stop()

st.sidebar.success(f"Loaded {len(df)} rows")

# ---------- Basic cleaning ----------
def safe_text_col(df, col):
    if col in df.columns:
        return df[col].astype(str).fillna("")
    else:
        return pd.Series([""] * len(df))

# Clean review text (store in clean_review)
if "clean_review" not in df.columns:
    df["clean_review"] = safe_text_col(df, "review")
    df["clean_review"] = df["clean_review"].str.replace(r"http\S+|www\S+|https\S+", "", regex=True)
    df["clean_review"] = df["clean_review"].str.replace(r"[^A-Za-z0-9\s']", " ", regex=True)
    df["clean_review"] = df["clean_review"].str.replace(r"\s+", " ", regex=True).str.strip().str.lower()

# Sentiment derivation: use predicted_sentiment if available; else try from rating
if "predicted_sentiment" not in df.columns:
    if "rating" in df.columns:
        def rating_to_sentiment(r):
            try:
                r = float(r)
            except:
                return "Neutral"
            if r >= 4:
                return "Positive"
            if r == 3:
                return "Neutral"
            return "Negative"
        df["predicted_sentiment"] = df["rating"].apply(rating_to_sentiment)
    else:
        df["predicted_sentiment"] = "Neutral"

# Standardize sentiment values
df["predicted_sentiment"] = df["predicted_sentiment"].astype(str).str.title()

# Review length (words)
df["review_length"] = df["clean_review"].astype(str).apply(lambda x: len(str(x).split()))

# Date parsing (heuristic)
date_col = None
for c in df.columns:
    if "date" in c.lower() or "time" in c.lower():
        date_col = c
        break
if date_col:
    df["_dt"] = pd.to_datetime(df[date_col], errors="coerce")
else:
    df["_dt"] = pd.NaT

# -------------------------
# Sidebar Filters
# -------------------------
st.sidebar.markdown("### Filters")
max_words = int(df["review_length"].max() or 1000)
min_len, max_len = st.sidebar.slider("Review length (words)", 0, max_words, (0, max_words))
sentiments_available = sorted(df["predicted_sentiment"].unique(), key=lambda s: ["Positive","Neutral","Negative"].index(s) if s in ["Positive","Neutral","Negative"] else 99)
sent_filter = st.sidebar.multiselect("Sentiment", options=sentiments_available, default=sentiments_available)
rows_to_load = st.sidebar.number_input("Limit rows used (0 = all)", min_value=0, max_value=len(df), value=0, step=50)
if rows_to_load > 0:
    df_work = df.head(int(rows_to_load)).copy()
else:
    df_work = df.copy()

df_f = df_work[(df_work["review_length"] >= min_len) & (df_work["review_length"] <= max_len) & (df_work["predicted_sentiment"].isin(sent_filter))]

# -------------------------
# Utility functions
# -------------------------
def plot_wordcloud(text, title=None, width=600, height=300, colormap="viridis"):
    if not text or str(text).strip() == "":
        st.write("No words to display")
        return
    wc = WordCloud(width=width, height=height, background_color="white", stopwords=EN_STOPWORDS, colormap=colormap).generate(text)
    plt.figure(figsize=(10,4))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    if title:
        plt.title(title)
    st.pyplot(plt.gcf())
    plt.clf()

def top_n_words(corpus, n=25, ngram_range=(1,2), min_df=2):
    if isinstance(corpus, (list, pd.Series)):
        corpus = list(corpus)
    else:
        corpus = [str(corpus)]
    try:
        vec = CountVectorizer(stop_words="english", ngram_range=ngram_range, min_df=min_df)
        X = vec.fit_transform(corpus)
        freqs = np.asarray(X.sum(axis=0)).ravel()
        if freqs.size == 0:
            return pd.DataFrame({"term": [], "count": []})
        idx = np.argsort(freqs)[::-1][:n]
        features = np.array(vec.get_feature_names_out())[idx]
        counts = freqs[idx]
        return pd.DataFrame({"term": features, "count": counts})
    except Exception:
        return pd.DataFrame({"term": [], "count": []})

# -------------------------
# Questions (full text included)
# -------------------------
QUESTIONS_FULL = {
    1: "1. What is the overall sentiment of user reviews? â†’ Classify each review as Positive, Neutral, or Negative, and compute their proportions.",
    2: "2. How does sentiment vary with rating? â†’ Compare sentiment distribution across star ratings.",
    3: "3. What are the keywords by sentiment? â†’ Show word clouds / frequent tokens per sentiment class.",
    4: "4. How does sentiment trend over time? â†’ Show sentiment proportions over time (daily/weekly/monthly).",
    5: "5. Verified users vs sentiment â†’ Compare sentiment distribution for verified vs non-verified purchases.",
    6: "6. Review length vs sentiment â†’ Analyze whether review length correlates with sentiment.",
    7: "7. Sentiment by location â†’ Which locations show more positive / negative reviews?",
    8: "8. Sentiment by platform â†’ Compare sentiment across platforms (android/ios/web/etc.).",
    9: "9. Sentiment by product version â†’ Which versions have higher positive or negative reviews?",
    10: "10. What are the most common negative feedback themes? â†’ Use topic modeling or keyword grouping to identify recurring pain points in negative reviews."
}

question = st.selectbox(
    "Pick a question to display (full question shown)",
    options=list(QUESTIONS_FULL.keys()),
    format_func=lambda x: QUESTIONS_FULL[x]
)

# Use placeholder to ensure full replacement when switching questions
output_placeholder = st.empty()

# -------------------------
# Handlers for each question
# -------------------------
def handle_q1(data):
    sent_counts = data["predicted_sentiment"].value_counts().reindex(["Positive","Neutral","Negative"], fill_value=0)
    proportions = (sent_counts / sent_counts.sum()).round(3)
    fig, ax = plt.subplots(figsize=(2,3))
    ax.pie(sent_counts, labels=sent_counts.index, autopct="%1.1f%%", startangle=90, colors=["#2ecc71","#f1c40f","#e74c3c"])
    ax.set_title("Sentiment Proportions")
    return {"figure": fig, "counts": sent_counts, "proportions": proportions}

def handle_q2(data):
    if "rating" not in data.columns:
        return "No 'rating' column in dataset."
    cross = pd.crosstab(data["rating"], data["predicted_sentiment"], normalize="index")*100
    fig, ax = plt.subplots(figsize=(8,4))
    sns.heatmap(cross.fillna(0), annot=True, fmt=".1f", cmap="YlGnBu", ax=ax)
    ax.set_xlabel("Sentiment"); ax.set_ylabel("Rating")
    plt.tight_layout()
    return {"figure": fig, "table": cross.round(2)}

def handle_q3(data):
    results = {}
    for s in ["Positive","Neutral","Negative"]:
        sub = data[data["predicted_sentiment"]==s]
        text = " ".join(sub["clean_review"].astype(str).tolist())
        twords = top_n_words(sub["clean_review"].astype(str), n=12, ngram_range=(1,2), min_df=1)
        results[s] = {"text": text, "top_words": twords}
    return results

def handle_q4(data):
    if data["_dt"].notnull().any():
        freq = st.sidebar.radio("Trend granularity", ("M","W","D"), index=0, key="trend_freq")
        trend = data.dropna(subset=["_dt"]).groupby([pd.Grouper(key="_dt", freq=freq), "predicted_sentiment"]).size().unstack(fill_value=0)
        trend_pct = trend.div(trend.sum(axis=1), axis=0)*100
        fig, ax = plt.subplots(figsize=(10,4))
        trend_pct.plot(ax=ax)
        ax.set_ylabel("Percent")
        ax.set_xlabel("Date")
        plt.tight_layout()
        return {"figure": fig, "table": trend_pct}
    else:
        return "No date/time column available."

def handle_q5(data):
    if "verified_purchase" not in data.columns:
        # simulate small sample if missing
        tmp = data.copy()
        tmp["verified_purchase"] = np.random.choice([True, False], size=len(tmp))
        data = tmp
    vtab = pd.crosstab(data["verified_purchase"].astype(str).str.title(), data["predicted_sentiment"], normalize="index")*100
    fig, ax = plt.subplots(figsize=(6,4))
    vtab.plot(kind="bar", stacked=True, ax=ax, colormap="Set2")
    ax.set_ylabel("Percent")
    plt.tight_layout()
    return {"figure": fig, "table": vtab.round(2)}

def handle_q6(data):
    if "review_length" not in data.columns:
        return "No review length info."
    length_stats = data.groupby("predicted_sentiment")["review_length"].describe()
    fig, ax = plt.subplots(figsize=(6,4))
    sns.boxplot(x="predicted_sentiment", y="review_length", data=data,
                order=["Positive","Neutral","Negative"], palette=["#2ecc71","#f1c40f","#e74c3c"], ax=ax)
    ax.set_ylabel("Review length (words)")
    plt.tight_layout()
    return {"figure": fig, "table": length_stats}

def handle_q7(data):
    if "location" not in data.columns:
        return "No 'location' column."
    loc_avg = data.groupby("location")["predicted_sentiment"].apply(lambda x: x.value_counts(normalize=True).mul(100)).unstack().fillna(0)
    fig, ax = plt.subplots(figsize=(10,6))
    loc_avg.sort_values(by="Positive", ascending=False).head(10)["Positive"].plot(kind="bar", ax=ax, color="#2ecc71")
    ax.set_ylabel("Percent Positive")
    plt.tight_layout()
    return {"figure": fig, "table": loc_avg}

def handle_q8(data):
    if "platform" not in data.columns:
        return "No 'platform' column."
    plat = pd.crosstab(data["platform"], data["predicted_sentiment"], normalize="index")*100
    fig, ax = plt.subplots(figsize=(6,4))
    plat.plot(kind="bar", stacked=True, ax=ax, colormap="Paired")
    plt.tight_layout()
    return {"figure": fig, "table": plat.round(2)}

def handle_q9(data):
    version_col = next((c for c in data.columns if "version" in c.lower()), None)
    if not version_col:
        return "No 'version' column."
    version_table = pd.crosstab(data[version_col], data["predicted_sentiment"], normalize="index")*100
    fig, ax = plt.subplots(figsize=(10,6))
    version_table.sort_values(by="Positive", ascending=False).head(10)["Positive"].plot(kind="bar", ax=ax, color="#2ecc71")
    ax.set_ylabel("Percent Positive")
    plt.tight_layout()
    return {"figure": fig, "table": version_table}

def handle_q10(data):
    neg_texts = data[data["predicted_sentiment"]=="Negative"]["clean_review"].astype(str).tolist()
    if len(neg_texts) < 5:
        # fallback to keyword frequency
        top_neg = top_n_words(neg_texts, n=25, ngram_range=(1,2), min_df=1)
        return {"fallback": True, "n_negative": len(neg_texts), "top_neg": top_neg}
    # LDA parameters
    n_topics = st.slider("Number of LDA topics", 2, 8, 3, key="lda_topics")
    max_features = st.slider("LDA max features (vocab)", 500, 5000, 1500, step=500, key="lda_maxfeat")
    with st.spinner("Vectorizing negative reviews..."):
        vect = CountVectorizer(max_df=0.95, min_df=2, max_features=max_features, stop_words="english")
        X = vect.fit_transform(neg_texts)
    if X.shape[1] < 3:
        top_neg = top_n_words(neg_texts, n=25, ngram_range=(1,2), min_df=1)
        return {"fallback": True, "n_negative": len(neg_texts), "top_neg": top_neg}
    with st.spinner("Fitting LDA (this may take a moment)..."):
        lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, learning_method="batch", max_iter=10)
        lda.fit(X)
    feature_names = vect.get_feature_names_out()
    topics = {}
    for topic_idx, comp in enumerate(lda.components_):
        terms = [feature_names[i] for i in comp.argsort()[:-15:-1]]
        topics[f"Topic {topic_idx+1}"] = terms
    top_neg = top_n_words(neg_texts, n=25, ngram_range=(1,2), min_df=1)
    return {"fallback": False, "n_negative": len(neg_texts), "topics": topics, "top_neg": top_neg}

# Mapping handlers
HANDLERS = {
    1: handle_q1,
    2: handle_q2,
    3: handle_q3,
    4: handle_q4,
    5: handle_q5,
    6: handle_q6,
    7: handle_q7,
    8: handle_q8,
    9: handle_q9,
    10: handle_q10
}

# -------------------------
# Compute answer (cached on question and data slice size)
# -------------------------
@st.cache_data
def compute_answer(question_num, df_snapshot_hash, handler_key):
    """
    We pass a simple hash or integer derived from the dataframe slice so cache invalidates
    when filters / size change. To keep the cache safe, compute outside heavy objects.
    """
    # The handler_key is a convenience to make cached signature dependent on handler config (if present)
    handler = HANDLERS[question_num]
    # For safety, re-load the dataframe inside the function will be done by caller; here we only return the handler itself
    return handler

# Create a lightweight fingerprint so cache is tied to the current filtered dataset size & hash
df_fingerprint = (len(df_f), int(df_f["review_length"].sum()) if "review_length" in df_f.columns else 0)
handler_ref = compute_answer(question, df_fingerprint, question)

# Now actually run the handler (not cached) - handlers may use st.slider inside (q10) so must run in main thread
result = HANDLERS[question](df_f)

# -------------------------
# Render output
# -------------------------
with output_placeholder.container():
    st.subheader(QUESTIONS_FULL[question])
    if isinstance(result, dict) and "figure" in result:
        st.pyplot(result["figure"])
        if "table" in result:
            st.markdown("**Table**")
            st.dataframe(result["table"].fillna(0).round(3))
        if "counts" in result:
            st.markdown("**Counts**")
            st.dataframe(result["counts"].rename_axis("sentiment").reset_index().rename(columns={0:"count"}))
        if "proportions" in result:
            st.markdown("**Proportions**")
            st.dataframe(result["proportions"].rename_axis("sentiment").reset_index().rename(columns={0:"proportion"}))
    elif isinstance(result, dict) and "fallback" in result:
        st.write(f"Negative reviews analyzed: {result.get('n_negative')}")
        if result["fallback"]:
            st.write("Data too small for LDA â€” top negative terms (fallback):")
            st.dataframe(result["top_neg"])
        else:
            st.write("LDA topics:")
            for t, words in result["topics"].items():
                st.markdown(f"**{t}:** " + ", ".join(words))
            st.subheader("Top negative terms")
            st.dataframe(result["top_neg"])
    elif isinstance(result, dict):
        # Generic dict (e.g., q3 keywords)
        for k, v in result.items():
            st.subheader(str(k))
            if isinstance(v, dict) and "text" in v and "top_words" in v:
                st.markdown(f"**{k} - Word Cloud**")
                plot_wordcloud(v["text"], title=f"Top words â€” {k}", colormap={"Positive":"Greens","Neutral":"Purples","Negative":"Reds"}.get(k, "viridis"))
                st.markdown("**Top words (table)**")
                st.dataframe(v["top_words"])
            else:
                st.write(v)
    elif isinstance(result, str):
        st.write(result)
    elif isinstance(result, plt.Figure):
        st.pyplot(result)
    elif isinstance(result, pd.DataFrame):
        st.dataframe(result)
    else:
        st.write(result)

# Footer
st.markdown("---")
st.sidebar.write(f"Selected question: {question} â€” {QUESTIONS_FULL[question].split('â†’')[0].strip()}")
